{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35a28a4",
   "metadata": {},
   "source": [
    "# CFRM 521 - Final Report\n",
    "\n",
    "#### By: Wooseok (Jeff), Max, Steve, Ilse, Jasmine\n",
    "\n",
    "----\n",
    "\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75e0d6",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the packages (at the end we can consolidate all the packages we individually used here so that we can create a single environment.yml file for a conda environment at the end)\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import uniform, loguniform\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Read in the data (this section should be the same for everyone)\n",
    "url = \"https://api.github.com/repos/stevedemirev/CFRM521-ProjectData/contents/filtered\"\n",
    "response = requests.get(url)\n",
    "files = response.json()\n",
    "\n",
    "csv_files = sorted([file for file in files if file['name'].endswith('.csv')], key = lambda x: x['name'])\n",
    "\n",
    "def get_datasets(files):\n",
    "    df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        temp = pd.read_csv(file['download_url'])\n",
    "        df = pd.concat([df, temp], ignore_index = True)\n",
    "    return df\n",
    "\n",
    "full_df = get_datasets(csv_files)\n",
    "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Filter to Calls Only\n",
    "full_df = full_df[full_df['type'] == \"call\"]\n",
    "# Filter to every 10th row\n",
    "full_df = full_df.iloc[::10]\n",
    "\n",
    "total_files = len(full_df)\n",
    "train_size = int(total_files*0.7)\n",
    "val_size = int(total_files*0.85)\n",
    "\n",
    "train = full_df[:train_size]\n",
    "valid = full_df[train_size:val_size]\n",
    "test = full_df[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f154f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train) + len(valid) + len(test)\n",
    "def proportion(df):\n",
    "    return round(len(df)/total_size,3)\n",
    "    \n",
    "print(f\"Length of Training set: {len(train):,} rows, Proportion: {proportion(train)}\")\n",
    "print(f\"Length of Validation set: {len(valid):,} rows, Proportion: {proportion(valid)}\")\n",
    "print(f\"Length of Testing set: {len(test):,} rows, Proportion: {proportion(test)}\")\n",
    "print(f\"Original Dataset size: {total_size:,} rows, Sum Check: {proportion(train)+proportion(valid)+proportion(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e9ad7",
   "metadata": {},
   "source": [
    "# Max Black: SVR\n",
    "\n",
    "The model for this section will be support vector regression. We chose support vector regression because we are working with options pricing and expect to see complex non-linear relationships. Support vector regression is an excellent candidate for this task, but does have a few drawbacks. Support vector regression scales very slowly with the number of data entries, which means we are limited by our hardware for the purposes of this project. Running on more than around 30,000 data points triggers runtimes that makes optimizing hyperparameters extremely difficult. \n",
    "\n",
    "Prior to training our support vector regressor, we will train a naive model that predicts each point to be the mean of our training data. We expect our fully fitted and optimized support vector regressor to outperform this considerably. In order to show this, we will be using mean squared error as our core loss function, but will also examine mean absolute error so we can tell how much outliers are impacting the results. We will also analyze residuals to determine to better understand overall model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['close', 'strike', 'delta', 'gamma',\n",
    "                'vega', 'theta', 'implied_volatility', 'time_to_expiration']\n",
    "\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['mid_price']\n",
    "\n",
    "X_valid = valid[feature_cols]\n",
    "y_valid = valid['mid_price']\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['mid_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdcc14",
   "metadata": {},
   "source": [
    "We will start by training a scaler on the training data, and applying it to the validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c87561",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1f8f5",
   "metadata": {},
   "source": [
    "## Naive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff23bf",
   "metadata": {},
   "source": [
    "To determine the efficacy of our support vector regression we will first train a naive model using the mean of our training data. As mentioned before, this model predicts every point to be the mean of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3f645a",
   "metadata": {},
   "source": [
    "Below is a function to plot learning curves for mean absolute error and root mean squared error. It is based on a function introduced in lecture, but adjusted to reduce the number of plotted points. Support vector regression is a very slow process, and plotting a learning curve with a different training set for every entry in the training set, which is what the original function did, is impractical, especially considering the size of our data. For the naive model though, this is not a problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e77635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X_train, y_train, X_val, y_val, n_points=50):\n",
    "    train_sizes = np.linspace(10, len(X_train), n_points, dtype=int)\n",
    "    \n",
    "    train_rmse, val_rmse = [], []\n",
    "    train_mae, val_mae = [], []\n",
    "\n",
    "    for m in train_sizes:\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train[:m], y_train_predict)))\n",
    "        val_rmse.append(np.sqrt(mean_squared_error(y_val, y_val_predict)))\n",
    "\n",
    "        train_mae.append(mean_absolute_error(y_train[:m], y_train_predict))\n",
    "        val_mae.append(mean_absolute_error(y_val, y_val_predict))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 7))\n",
    "\n",
    "    axs[0].plot(train_sizes, train_rmse, \"r-+\", linewidth=2, label=\"Train\")\n",
    "    axs[0].plot(train_sizes, val_rmse, \"b-\", linewidth=3, label=\"Validation\")\n",
    "    axs[0].set_title(\"Learning Curve (RMSE)\", fontsize=14)\n",
    "    axs[0].set_xlabel(\"Training set size\", fontsize=12)\n",
    "    axs[0].set_ylabel(\"RMSE\", fontsize=12)\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    axs[1].plot(train_sizes, train_mae, \"r-+\", linewidth=2, label=\"Train\")\n",
    "    axs[1].plot(train_sizes, val_mae, \"b-\", linewidth=3, label=\"Validation\")\n",
    "    axs[1].set_title(\"Learning Curve (MAE)\", fontsize=14)\n",
    "    axs[1].set_xlabel(\"Training set size\", fontsize=12)\n",
    "    axs[1].set_ylabel(\"MAE\", fontsize=12)\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4f79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(dummy, X_train_scaled, y_train, X_valid_scaled, y_valid, n_points=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839361b",
   "metadata": {},
   "source": [
    "From these results, we can clearly see that the naive model is underfitting the data. The RMSE and MAE do appear to converge, but to large values of RMSE and MAE. \n",
    "\n",
    "Below we plot the actual and predicted values of the naive model along with the MSE and MAE of our naive model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_dummy = dummy.predict(X_test_scaled)\n",
    "\n",
    "mse_dummy = mean_squared_error(y_test, y_pred_test_dummy)\n",
    "mae_dummy = mean_absolute_error(y_test, y_pred_test_dummy)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse_dummy:.4f}\")\n",
    "print(f\"Test MAE: {mae_dummy:.4f}\")\n",
    "\n",
    "plt.scatter(y_test, y_pred_test_dummy)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Option Mid-Prices\")\n",
    "plt.ylabel(\"Predicted Option Mid-Prices\")\n",
    "plt.title(\"Prediction Vs Actual Option Mid-Prices\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5baea",
   "metadata": {},
   "source": [
    "The predicted prices are constant throughout as expected. Clearly this does a poor job of estimating our option prices. We can further see that the test MSE and MAE are extremely high. \n",
    "\n",
    "Below we plot the distribution of the model residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_dummy = y_test - y_pred_test_dummy\n",
    "plt.hist(residuals_dummy, bins=50, density=True)\n",
    "plt.title(\"Residual Distribution for Naive Model\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06100388",
   "metadata": {},
   "source": [
    "We predict the mean, so we would expect the residuals to cluster around 0, which they do. It does appear to be negatively skewed, which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(residuals_dummy, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Residuals for Naive Model\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea3d3f1",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0270bad",
   "metadata": {},
   "source": [
    "We will run a quick pilot study to narrow our search down to one kernel. Ideally would would explore each of them more, but given the complexity of our data structure we are confident that the RBF kernel will be most effective. This is most commonly used when pricing options with SVR, as seen in Wang and Zhang (2010) and Andreou et Alias (2009). We will test each kernel with default hyperparameters to ensure that the RBF kernel is outperforming other kernels, but will not examine this further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = ['linear', 'poly', 'sigmoid', 'rbf',]\n",
    "results = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(f\"\\nSVR with {kernel} Kernel\")\n",
    "\n",
    "    model = SVR(kernel=kernel, C=1.0, epsilon=0.1)\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_valid_scaled)\n",
    "\n",
    "    mse = mean_squared_error(y_valid, y_pred)\n",
    "\n",
    "    mae = mean_absolute_error(y_valid, y_pred)\n",
    "\n",
    "    results[kernel] = {'MSE': mse, 'MAE': mae}\n",
    "    print(f\"Validation MSE: {mse:.4f}\")\n",
    "    print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798408e",
   "metadata": {},
   "source": [
    "Based on these results, we can see that our SVR performed best using the RBF kernel, as we expected. The difference is significant by all metrics.\n",
    "\n",
    "Now that we have analyzed our naive RBF kernel model we will move on to tuning the hyperparameters to see if it significantly improves the accuracy. For hyperparameter tuning we are going to utilize the random search method. We are working with 3 parameters that can take on a continuous range of values. To briefly summarize each of the parameters\n",
    "\n",
    "- $C$ controls the degree to which margin violations are penalized\n",
    "- $\\varepsilon$ controls the margin of error that we ignore\n",
    "- $\\gamma$ controls how 'local' the decision boundary is\n",
    "\n",
    "Each of three parameters can take the value of any postive real number. Below we test a range of these values with the RBF kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rbf = SVR(kernel='rbf')\n",
    "\n",
    "param_distributions = {\n",
    "    'C': loguniform(1e-1, 1e3),       \n",
    "    'gamma': loguniform(1e-4, 1),      \n",
    "    'epsilon': uniform(0.01, 0.3)    \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    model_rbf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,             \n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,                    \n",
    "    n_jobs=-1,              \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters Found:\")\n",
    "print(search.best_params_)\n",
    "\n",
    "print(\"\\nBest CV MSE:\", -search.best_score_)\n",
    "\n",
    "y_pred = search.predict(X_valid_scaled)\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "\n",
    "print(f\"\\nValidation MSE: {mse:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ab043",
   "metadata": {},
   "source": [
    "As we can see the high $C$ value, this model penalizes errors severely. Still it selects a relatively high epsilon of 0.3, so it accepts a small margin of error. The model selects a moderate gamma, not making decisions boundary too local, and allowing the model to generalize slightly more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab39d2",
   "metadata": {},
   "source": [
    "Below we call the function to generate learning curves for our SVR regressor with the RBF kernel and optimized hyperparameters. We can see already from the result above that this model performs well on the validation sets, with MSE of less than 1 and MAE error of less than 0.3. Recall from our naive model that the MSE was over 2000, so this result is encouraging. Note that the validation data shown in the plots below is from the mean across our 3 cross-validation folds in the training data, not the validation dataset that we seperated at the beginning. The calidation predictions are meant to give us some idea about model performance at this stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_plot = SVR(kernel='rbf', **search.best_params_)\n",
    "\n",
    "plot_learning_curves(best_model_plot, X_train_scaled, y_train, X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d681b",
   "metadata": {},
   "source": [
    "Now we can determine the results of running this support vector regression on our test data. \n",
    "\n",
    "We can see that the errors decrease rapidly as training size increases, with a sharp elbow at aroun 1000 data points. The validation and training sets RMSE and MAE converge to very lower values, with minimal seperation between the two lines. This indicates that our model is neither overfitting or underfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea15e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = search\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nTest MSE: {mse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa7cc73",
   "metadata": {},
   "source": [
    "The results from our test data show a MSE of less then when, which is a strong result. We saw that our naive data had a MSE of over 2000, so clearly the support vector regressor with RBF kernel is outperformming that. We will now look at how the residuals from the support vector regressor behave. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred_test, alpha = 0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Option Mid-Prices\")\n",
    "plt.ylabel(\"Predicted Option Mid-Prices\")\n",
    "plt.title(\"Prediction Vs Actual Option Mid-Prices\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dbc59f",
   "metadata": {},
   "source": [
    "From this plot of the predicted vs actual option prices we can see that the every point is close to the actual values. There appears to be some more variance in results closer to zero, but for higher prices the results appear to perform consistently very well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ade7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_test\n",
    "plt.hist(residuals, bins=50, density=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08833e6c",
   "metadata": {},
   "source": [
    "From this plot of our residuals, we can see that the vast majority are clustered around zero. This is a good sign, and tells use that our support vector regression likely does not have a positive or negative skew. \n",
    "\n",
    "Below we also plot a QQ-plot. Note that there is no assumption of normality of error terms in support vector regression, like there is in linear regression, so we dont actually expect the errors to be normally distributed. We are only doing this to better understand the distribution of the residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c138dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Residuals for SVR\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdfdecb",
   "metadata": {},
   "source": [
    "We can see that there are significantly heavier postive and negative tails than you would typically see in a normal distribution. This makes sense, because the model is indifferent to small residuals that fall inside the band created by the hyperparameter $\\varepsilon$. This means that residuals can spread out freely near the edge of this bounadary, making the distribution have significantly heavier tails than a normal distribution. This behavior is expected, but is a downside of the model. The width of epsilon could become more significant of a factor for options that have a low prices point. In fact we saw that to some degree when we plotted the actual and predicted values against one another. At lower price points there was a more obvious spread, which suggests that this band was having a noticable impact on the predicted price. We need to be very careful when using it to price extremely low price options, because in those price ranges the band created by epsilon is much more noticable. \n",
    "\n",
    "Overall this model is very promising, but has could have issues modeling options in low price ranges. Still it performs very constitently throughout the training, validation and test sets, and generates a low MSE and MAE for the test set, which indicates it is able to price these options effectively. There are also no extreme outliers in our results, which tells us this model performs consistently. \n",
    "\n",
    "## References\n",
    "\n",
    "Wang, J., & Zhang, J. (2010). Stock trend prediction based on a new status box method. Expert Systems with Applications, 37(8), 5640–5649. https://doi.org/10.1016/j.eswa.2010.02.094\n",
    "\n",
    "Andreou, P. C., Charalambous, C., & Martzoukos, S. H. (2010). European option pricing by using the support vector regression approach. In Artificial Intelligence Applications and Innovations (pp. 247–256). Springer. https://doi.org/10.1007/978-3-642-16239-8_26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224a522",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Wooseok's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7336c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf500532",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Steve's Model\n",
    "\n",
    "## Baseline Model\n",
    "\n",
    "For my model's architecture, I decided to use 4 hidden layers with 50 neurons each with 'ReLU' activation, 'he_normal' initializer, 'Nadam' optimizer, early stopping with a patience of 10, and \"MSE\" as the primary loss metric, and \"MAE\" as the secondary loss metric. The choice to use 4 hidden layers with 50 neurons was somewhat arbitrary and not backed by any theoretical justification other than the expectation of it being able to accurately capture any non-linear relationships occuring with options pricing. The output layer would remain the default linear activation function with one neuron as we are expecting a single value, the mid price. I also chose the \"ReLU\" activation function for each neuron with \"he_normal\" initializer to better model the non-linear relationship and avoid vanishing gradients during training. Additionally, \"ReLU\" works well because it encourages positive only outputs, which matches with the intrinsic value of an option is defined as: $V = \\text{max}(S_T - K, 0)$. The 'Nadam' optimizer was chosen here as its learning rate is adaptable, allowing it to converge faster and smoother for non-linear functions compared to other optimizers. Early stopping was added as a form of regularization to prevent unnecessary training once the model stops improving on the validation set. Given the time required to train the model, early stopping is an effective way to reduce training time while also helping to avoid overfitting. Since the objective of this project is to predict option prices, Mean Squared Error (MSE) and Mean Absolute Error (MAE) are appropriate evaluation metrics. MSE penalizes larger errors more heavily, making it useful for identifying significant prediction deviations, while MAE provides a more interpretable measure of the average prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_features(df):\n",
    "    df['expiration'] = pd.to_datetime(df['expiration'])\n",
    "    df['quote_date'] = pd.to_datetime(df['quote_date'])\n",
    "    df['tte'] = (df['expiration'] - df['quote_date']).dt.days / 252\n",
    "    X = df[['close', 'strike', 'delta', 'gamma',\n",
    "            'vega', 'theta', 'implied_volatility', 'tte']]\n",
    "    y = df['mid_price']\n",
    "    return X, y\n",
    "\n",
    "def get_features():\n",
    "    X_feats = []\n",
    "    y_feats = []\n",
    "    for df in [train, valid, test]:\n",
    "        df = df.copy()\n",
    "        X, y = define_features(df)\n",
    "        X_feats.append(X)\n",
    "        y_feats.append(y)\n",
    "    return X_feats, y_feats\n",
    "\n",
    "X_feats, y_feats = get_features()\n",
    "X_train, X_valid, X_test = X_feats\n",
    "y_train, y_valid, y_test = y_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, val, test):\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    valid_scaled = scaler.transform(val)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return train_scaled, valid_scaled, test_scaled\n",
    "\n",
    "def reset_session(seed=42):\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def build_model(input_shape):\n",
    "    reset_session()\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape = (input_shape,)),\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\", kernel_initializer = \"he_normal\"),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = \"nadam\",\n",
    "        loss = \"mse\",\n",
    "        metrics = ['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "X_train_scaled, X_valid_scaled, X_test_scaled = scale_data(X_train, X_valid, X_test)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    patience = 10,\n",
    "    mode = \"min\",\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X_train_scaled.shape[1])\n",
    "\n",
    "start = time.time()\n",
    "history_baseline = model.fit(X_train_scaled, y_train,\n",
    "             validation_data = (X_valid_scaled, y_valid),\n",
    "             epochs = 50, verbose = 1, callbacks=[early_stop])\n",
    "end = time.time()\n",
    "baseline_time = end-start\n",
    "\n",
    "mse_val, mae_val = model.evaluate(X_valid_scaled,\n",
    "                                        y_valid, verbose = 0)\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled,\n",
    "                                        y_test, verbose = 0)\n",
    "\n",
    "print(\"\\Baseline Model:\")\n",
    "print(f\"Validation MSE: {mse_val}\")\n",
    "print(f\"Validation MAE: {mae_val}\")\n",
    "\n",
    "print(f\"\\nTest MSE: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c507f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 6))\n",
    "\n",
    "    ax1.plot(history.epoch, history.history[\"loss\"], '*-', label=\"Training Loss\", color='b')\n",
    "    ax1.plot(history.epoch, history.history[\"val_loss\"], '--', label=\"Validation Loss\", color='b')\n",
    "    ax1.set_title(\"Training vs Validation Loss (MSE) for Call Options\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss (MSE)\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.plot(history.epoch, history.history[\"mae\"], '*-', label=\"Train MAE\", color='r')\n",
    "    ax2.plot(history.epoch, history.history[\"val_mae\"], '--', label=\"Val MAE\", color='r')\n",
    "    ax2.set_title(\"Training vs Validation MAE for Call Options\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Mean Absolute Error\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd88ca",
   "metadata": {},
   "source": [
    "As we can see from the learning curves above, the training loss and the validation loss converge closely, suggesting that the model is learning effectively without overfitting. We can also confirm this by looking at the training and validation MAE which also converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_plot(model, X_test_s, y_test):\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    \n",
    "    residuals = y_test - y_pred.flatten()\n",
    "    plt.scatter(y_test, y_pred, marker = 'o', alpha = 0.3)\n",
    "    plt.plot([y_test.min(), y_test.max()], \n",
    "             [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel(\"Actual Option Mid-Prices\")\n",
    "    plt.ylabel(\"Predicted Option Mid-Prices\")\n",
    "    plt.title(f\"Prediction Vs Actual Call Option Mid-Prices\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return residuals\n",
    "\n",
    "residuals = get_prediction_plot(model, X_test_scaled, \n",
    "                    y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1859d",
   "metadata": {},
   "source": [
    "This plot allows us to visualize how well the model predicted the mid-price of the option, with a perfectly diagonal line indicating all the predictions were completely accurate. The model does seem to follow this diagonal line but there is a noticeable deviation around 0 where predictions seem to be less accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.name = \"residuals\"\n",
    "display(residuals.to_frame().describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8ac4b",
   "metadata": {},
   "source": [
    "The summary statistics of the residuals show that the model’s average error is close to zero, indicating that, on average, the predicted option prices closely match the actual values. The standard deviation of approximately 1.26 suggests that most prediction errors fall within ±1.26 of the true option price. However, the model’s largest error exceeded 33 dollars over prediction, revealing the presence of significant outliers and suggesting there is room for further improvement in handling extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True, bins = 1000)\n",
    "plt.axvline(residuals.mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlim(-3,3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f36614",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(residuals, dist = \"norm\", plot = plt)\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91031def",
   "metadata": {},
   "source": [
    "As we can see from the two plots above, while the histogram looks normally distributed, the QQ-plot tells us that it is in fact not normally distributed. If the residuals were normally distributed, then it would form a diagonal line, but here the tail ends significantly deviate from the diagonal line, indicating that the tails are heavier than those of a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef66b47",
   "metadata": {},
   "source": [
    "## Hyperparameter search\n",
    "\n",
    "To optimize the Neural Network model, we want to identify the best hyperparameter configuration which predicts the option's `mid_price` closest, to do this I will implement a randomized search. \n",
    "\n",
    "A few of the hyperparameters to optimize are: \n",
    "* Number of Hidden Layers\n",
    "* Number of Neurons per layer\n",
    "* Neuron activation function\n",
    "* Neuron l2 regularization\n",
    "* Learning rate\n",
    "* Optimizer\n",
    "\n",
    "To find the best configuration, we will select the number of hidden layers between 1 and 5, as adding more may be redundant. For the number of neurons, we will select a value between 10 and 100 neurons. Since deep neural networks often perform better with 'swish' activation rather than with 'ReLU', we will select the activation function between those two at random to assess which performs better. Our l2 regularizer and learning rate will both be chosen from a range between 1e-6 and 1e-2 to allow a broader range of values in which to assess performance on. Lastly, the optimizer will be selected at random as either the 'Adam' optimizer or the adam optimizer with nesterov momentum ('Nadam') to assess which fits the model better. Once again we will include early stopping to avoid unneccessary training, but this time with a patience of 5, as we will only be training on 20 epochs instead of 50 due to time constraints. This randomized search will run for 30 iterations in order to assess the optimal parameters, which we will then compare the best model here against the baseline model on their test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b345f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "def build_model(hp):\n",
    "    reset_session()\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(shape = (X_train.shape[1],)))\n",
    "\n",
    "    n_hidden = hp.Int(\"n_hidden\", 1, 5)\n",
    "    for i in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            hp.Int(f\"n_neurons_{i+1}\", 10, 100),\n",
    "            activation = hp.Choice(\"activation\", [\"relu\", 'swish']),\n",
    "            kernel_regularizer = tf.keras.regularizers.l2(\n",
    "            hp.Float(\"l2\", 1e-6, 1e-2, sampling=\"log\")\n",
    "            ),\n",
    "            kernel_initializer = \"he_normal\"\n",
    "        ))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    learning_rate = hp.Float(\"learning_rate\", 1e-6, 1e-2, sampling = \"log\")\n",
    "    optimizer_name = hp.Choice(\"optimizer\", [\"Adam\", \"Nadam\"])\n",
    "    \n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "  \n",
    "    model.compile(loss=\"mse\", metrics = ['mae'], optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    patience = 5,\n",
    "    mode = \"min\",\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, \n",
    "    objective = \"val_loss\", \n",
    "    seed = 42, \n",
    "    max_trials = 30, \n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "random_search_tuner.search(X_train_scaled, y_train, epochs = 20, \n",
    "                    validation_data = (X_valid_scaled, y_valid), verbose = 1,\n",
    "                          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = random_search_tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "best_hyperparams.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fdd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(best_hyperparams)\n",
    "\n",
    "start = time.time()\n",
    "history_best = best_model.fit(X_train_scaled, y_train,\n",
    "             validation_data = (X_valid_scaled, y_valid),\n",
    "             epochs = 50, verbose = 1, callbacks=[early_stop])\n",
    "end = time.time()\n",
    "tuned_model_time = end - start\n",
    "\n",
    "test_loss_best, test_mae_best = best_model.evaluate(X_test_scaled, y_test)\n",
    "print(\"\\nTest MSE:\", test_loss_best)\n",
    "print(\"Test MAE:\", test_mae_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e80a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(history_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c158991",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = get_prediction_plot(best_model, X_test_scaled, \n",
    "                    y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650194ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, density=True, bins = 1000)\n",
    "plt.axvline(residuals.mean(), color='r', linestyle='--', label='Mean')\n",
    "plt.xlim(-3,3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ae1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(residuals, dist = \"norm\", plot = plt)\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline',\n",
    "        'Layers': 4,\n",
    "        'Activation': \"ReLU\",\n",
    "        'Optimizer': \"Nadam\",\n",
    "        'Epochs': 50,\n",
    "        'Test MAE': round(test_mae, 4),\n",
    "        'Test MSE': round(test_loss, 4),\n",
    "        'Train Time (s)': round(baseline_time, 2)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Tuned',\n",
    "        'Layers': int(best_hyperparams['n_hidden']),\n",
    "        'Activation': str(best_hyperparams['activation']),\n",
    "        'Optimizer': str(best_hyperparams['optimizer']),\n",
    "        'Epochs': max(history_best.epoch)+1,\n",
    "        'Test MAE': round(test_mae_best, 4),\n",
    "        'Test MSE': round(test_loss_best, 4),\n",
    "        'Train Time (s)': round(tuned_model_time, 2)\n",
    "    }\n",
    "])\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725080d9",
   "metadata": {},
   "source": [
    "As we can see from the plots above, the hyperparameter tuned model also doesn't seem to be overfitting as the training loss metrics converge nicely with the validation losses. The predictions seem to be slightly worse compared to the baseline model as we encounter more incorrectly estimated values around 0, this can be reinforced from the test MSE and test MAE which were higher compared to the baseline model. \n",
    "\n",
    "Regardless, the hyperparameter tuned model seemed to use the same activation function and optimizer as our baseline model did, possibly indicating these were better parameters than \"swish\" activation and \"adam\" optimizer we were evaluating against. The number of hidden layers in the tuned model was also less than the baseline model, which allowed for a slightly faster training speed at the cost of slightly worse predictions. \n",
    "\n",
    "In summary, the hyperparameter optimization did not yield a significant improvement over our initial model architecture. This suggests that the baseline model was already well-suited for the task. For future work, we could build upon the baseline model by performing another randomized search, but this time focusing only on optimizing the l2 regularization and the learning rate parameters which could further improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4a9d1",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Ilse's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58595e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "986b4e73",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Jasmine's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5cfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ab67c93",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1ecef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
